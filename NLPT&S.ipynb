{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMmAqhnOAQezM7nfA3MwkK3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khilav23/Translator-Summarizer/blob/main/NLPT%26S.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Requisites"
      ],
      "metadata": {
        "id": "k_tNQXk3C2ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "id": "yHHJ3E8n9H_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import LsiModel\n",
        "from gensim.corpora import Dictionary\n",
        "from googletrans import Translator\n",
        "from heapq import nlargest\n"
      ],
      "metadata": {
        "id": "iPvyHKc09qoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "o4QH4lGF_fFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translator and Summarizer program"
      ],
      "metadata": {
        "id": "T6oej_iRDFVG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDZ_yNAg81ay"
      },
      "outputs": [],
      "source": [
        "# Initializing the translator\n",
        "translator = Translator()\n",
        "\n",
        "text = \"\"\"\n",
        "BDavis et al. (2015) ont entrepris de tester empiriquement le dicton populaire « une pomme par jour éloigne le médecin ». Les pommes sont souvent utilisées pour représenter un mode de vie sain, et la recherche a montré que leurs propriétés nutritionnelles pourraient être bénéfiques pour divers aspects de la santé. L'approche unique des auteurs consiste à prendre le dicton au pied de la lettre et à se demander : les personnes qui mangent des pommes utilisent-elles moins fréquemment les services de santé ? S'il existe effectivement une telle relation, suggèrent-ils, la promotion de la consommation de pommes pourrait contribuer à réduire les coûts des soins de santé.\n",
        "\n",
        "L'étude a utilisé des données transversales accessibles au public de l'enquête nationale sur la santé et la nutrition. Les participants ont été classés comme consommateurs de pommes ou non-consommateurs de pommes en fonction de leur consommation de pommes sur une période moyenne de 24 heures. Ils ont également été classés comme ayant évité ou non l'utilisation des services de santé au cours de l'année écoulée. Les données ont été analysées statistiquement pour tester s'il existait une association entre la consommation de pommes et plusieurs variables dépendantes : les visites chez le médecin, les séjours à l'hôpital, l'utilisation des services de santé mentale et l'utilisation de médicaments sur ordonnance.\n",
        "\n",
        "Même si les consommateurs de pommes étaient légèrement plus susceptibles d'avoir évité les visites chez le médecin, cette relation n'était pas statistiquement significative après ajustement en fonction de divers facteurs pertinents. Aucune association n'a été trouvée entre la consommation de pommes et les séjours à l'hôpital ou l'utilisation des services de santé mentale. Cependant, les consommateurs de pommes étaient légèrement plus susceptibles d'avoir évité d'utiliser des médicaments sur ordonnance. Sur la base de ces résultats, les auteurs concluent qu'une pomme par jour n'éloigne pas le médecin, mais peut éloigner le pharmacien. Ils suggèrent que cette découverte pourrait avoir des implications pour la réduction des coûts des soins de santé, compte tenu des coûts annuels élevés des médicaments sur ordonnance et du faible coût des pommes.\n",
        "\n",
        "Cependant, les auteurs notent également plusieurs limites de l'étude : le plus important, c'est que les consommateurs de pommes sont susceptibles de différer des personnes qui n'ont pas consommé de pommes d'une manière qui peut avoir faussé les résultats (par exemple, les consommateurs de pommes peuvent être plus susceptibles d'être des consommateurs de santé -conscient). Pour établir une relation causale entre la consommation de pommes et l'évitement des médicaments, ils recommandent des recherches expérimentales.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Function to translate text to English\n",
        "#def translate_to_english(text):\n",
        "    #translated_text = translator.translate(text).text\n",
        "    #return translated_text\n",
        "\n",
        "# Function to preprocess text for LSA\n",
        "def preprocess_text(text):\n",
        "    # Tokenizing text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Removing punctuation and numbers\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "\n",
        "    # Removing stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatizing tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Function to summarize text using LSA\n",
        "def summarize(translated_text, num_sentences=3):\n",
        "    # Tokenizing the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Removing stop words and tokenize the remaining words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Calculating the frequency of each word\n",
        "    word_freq = {}\n",
        "    for word in filtered_words:\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "\n",
        "    # Calculating the weighted frequency of each sentence\n",
        "    sent_scores = {}\n",
        "    for sentence in sentences:\n",
        "        for word in word_tokenize(sentence.lower()):\n",
        "            if word in word_freq:\n",
        "                if len(sentence.split(' ')) < 30:\n",
        "                    if sentence in sent_scores:\n",
        "                        sent_scores[sentence] += word_freq[word]\n",
        "                    else:\n",
        "                        sent_scores[sentence] = word_freq[word]\n",
        "\n",
        "    # Selecting the top N sentences based on their scores\n",
        "    top_sentences = nlargest(num_sentences, sent_scores, key=sent_scores.get)\n",
        "\n",
        "    # Joining the top sentences into a summary\n",
        "    summary = ' '.join(top_sentences)\n",
        "    return summary\n",
        "\n",
        "#Summarize input text\n",
        "translated_text = Translator(text)\n",
        "summary = Translator(summarize(translated_text))\n",
        "print(\"Original Text:\\n\", text)\n",
        "print(\"Translated Text:\\n\", translated_text)\n",
        "print(\"\\nSummary:\\n\", summary)"
      ]
    }
  ]
}